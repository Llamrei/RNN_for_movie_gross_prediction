{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Follows \n",
    "https://www.tensorflow.org/tutorials/text/text_classification_rnn\n",
    "and \n",
    "https://www.tensorflow.org/tutorials/text/classify_text_with_bert\n",
    "\"\"\"\n",
    "import glob\n",
    "import os\n",
    "import pickle as pkl\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "# For pre-trained embeddings:\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track when we did the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-03 19:38:22.668781\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much it loads into memory for sampling - we're ok just loading everything at once as it isn't a lot of data\n",
    "BUFFER_SIZE = 10000\n",
    "# Batch for gradient averaging\n",
    "BATCH_SIZE = 64\n",
    "# Specify encoding of words\n",
    "SUBSET_VOCAB_SIZE = 5000\n",
    "START = start.strftime('%Y-%m-%d-%H%M')\n",
    "OUTPUT_DIR = Path(f'/rds/general/user/al3615/home/RNN_for_movie_gross_prediction/jobs/pretrained_embeddings/outputs/{START}')\n",
    "Path.mkdir(OUTPUT_DIR)\n",
    "GROSS_SYNOPSES_PATH = Path('/rds/general/user/al3615/home/RNN_for_movie_gross_prediction/complete10000_films_and_synopsis.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, metric, prefix=''):\n",
    "    if prefix:\n",
    "        prefix = f\"{prefix}-\"\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(8.0, 5.0))\n",
    "    \n",
    "    color = \"tab:red\"\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(metric, color=color)\n",
    "    ax1.plot(history.history[metric], color=color)\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color = \"tab:blue\"\n",
    "    ax2.set_ylabel(\n",
    "        \"val_\" + metric, color=color\n",
    "    )  # we already handled the x-label with ax1\n",
    "    ax2.plot(history.history[\"val_\" + metric], color=color)\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.legend([metric, \"val_\" + metric])\n",
    "    plt.savefig(OUTPUT_DIR/f\"{prefix}{metric}_history.png\", dpi=300)\n",
    "\n",
    "\n",
    "def split_data_into_input_and_output(data):\n",
    "    \"\"\"Take given data of format from scraper [link] and return the inputs and outputs seperated.\n",
    "\n",
    "    Args:\n",
    "        data (list): A numpy array/list of named tuples which contains entries for 'gross',\n",
    "        'title', 'synopsis' and 'year'.\n",
    "    \"\"\"\n",
    "    # Did this cause wanted to be certain about preserving order but i think numpy does that for free\n",
    "    data_in, data_out = list(zip(*[((x[\"synopsis\"]), x[\"gross\"]) for x in data]))\n",
    "    return np.array(data_in), np.array(data_out)\n",
    "\n",
    "\n",
    "def add_signal(data):\n",
    "    \"\"\"\n",
    "    If the given data has no signal we cant fit a NN to it. As such, here we append how much the film grossed\n",
    "    into the synopsis of each title.\n",
    "\n",
    "    Args:\n",
    "        data (list): A numpy array/list of named tuples which contains entries for 'gross',\n",
    "        'title', 'synopsis' and 'year'.\n",
    "    \"\"\"\n",
    "    for row in data:\n",
    "        row[\"synopsis\"] = row[\"synopsis\"] + f' The film grossed ${row[\"gross\"]}'\n",
    "\n",
    "\n",
    "def clean_copy(data, min_length=10, max_length=50, min_earning=0,max_earning=np.exp(30)):\n",
    "    cleaned_data = np.fromiter(\n",
    "        (x for x in data if len(x[\"synopsis\"].split()) > min_length), dtype=data.dtype\n",
    "    )\n",
    "    print(\n",
    "        f\"Crushed {len(data)} to {len(cleaned_data)} after removing sub {min_length} word synopses\"\n",
    "    )\n",
    "    old_len = len(cleaned_data)\n",
    "\n",
    "    cleaned_data = np.fromiter(\n",
    "        (x for x in cleaned_data if len(x[\"synopsis\"].split()) < max_length), dtype=data.dtype\n",
    "    )\n",
    "    print(\n",
    "        f\"Crushed {old_len} to {len(cleaned_data)} after removing super {max_length} word synopses\"\n",
    "    )\n",
    "    old_len = len(cleaned_data)\n",
    "\n",
    "    cleaned_data = np.fromiter(\n",
    "        (x for x in cleaned_data if x['gross'] > min_earning), dtype=data.dtype\n",
    "    )\n",
    "    print(\n",
    "        f\"Crushed {old_len} to {len(cleaned_data)} after removing sub {min_earning} gross\"\n",
    "    )\n",
    "    old_len = len(cleaned_data)\n",
    "\n",
    "    cleaned_data = np.fromiter(\n",
    "        (x for x in cleaned_data if x['gross'] < max_earning), dtype=data.dtype\n",
    "    )\n",
    "    print(\n",
    "        f\"Crushed {old_len} to {len(cleaned_data)} after removing super {max_earning} gross\"\n",
    "    )\n",
    "    old_len = len(cleaned_data)\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "def confusion_plot(lab, pred, name):\n",
    "    \"\"\"\n",
    "    Helper function to pile on scatter plots of labels and predictions that are real numbers\n",
    "    marking a helpful black line for the truth\n",
    "\n",
    "    lab = label\n",
    "    pred = prediction\n",
    "\n",
    "    needs a call to plt.show() or plt.savefig() after it cumulatively builds this\n",
    "    \"\"\"\n",
    "    plt.scatter(lab, lab, label=\"truth\", s=2, color=\"black\")\n",
    "    plt.scatter(lab, pred, label=name, s=2)\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys())\n",
    "    plt.xlabel(\"Truth\")\n",
    "    plt.ylabel(\"Prediction\")\n",
    "    \n",
    "def data_split(data, valid_fraction, test_fraction, train_fraction=None):\n",
    "    \"\"\"\n",
    "    Returns `data` split into (test_set, validation_set, training_set) where the \n",
    "    \"\"\"\n",
    "    \n",
    "    if train_fraction is None:\n",
    "        train_fraction = 1 - test_fraction - valid_fraction\n",
    "    rng = np.random.default_rng()\n",
    "    rng.shuffle(data)\n",
    "    len_d = len(data)\n",
    "    test_idx = int(len_d*test_fraction)\n",
    "    valid_idx = test_idx + int(len_d*valid_fraction)\n",
    "    # Just checking method is consistent\n",
    "    train_idx = valid_idx + int(len_d*train_fraction)\n",
    "    assert train_idx == len_d\n",
    "    return (data[:test_idx], data[test_idx:valid_idx], data[valid_idx:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, clean, shuffle, split and cast data into Tensorflow compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crushed 10000 to 9742 after removing sub 10 word synopses\n",
      "Crushed 9742 to 9209 after removing super 50 word synopses\n",
      "Crushed 9209 to 9209 after removing sub 0 gross\n",
      "Crushed 9209 to 9209 after removing super 10686474581524.463 gross\n"
     ]
    }
   ],
   "source": [
    "raw_data = pkl.load(open(GROSS_SYNOPSES_PATH, \"rb\"))\n",
    "data = clean_copy(raw_data)\n",
    "test, valid, train = data_split(raw_data, valid_fraction=0, test_fraction=0.15)\n",
    "\n",
    "# Fraction of overall data\n",
    "train_data_in, train_data_out = split_data_into_input_and_output(train)\n",
    "# valid_data_in, valid_data_out = split_data_into_input_and_output(valid)\n",
    "test_data_in, test_data_out = split_data_into_input_and_output(test)\n",
    "\n",
    "# Make dataset objects\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data_in, train_data_out))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data_in, test_data_out))\n",
    "\n",
    "# Prefetch parrallelising loading + execution (not huge so not necessary)\n",
    "# Dont need padded_batch as we are encoding and padding later\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(5)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(5)\n",
    "\n",
    "TRAIN_STEPS_PER_EPOCH = len(train_data_in) // BATCH_SIZE\n",
    "# VALID_STEPS_PER_EPOCH = len(valid_data_in) // BATCH_SIZE\n",
    "TEST_STEPS_PER_EPOCH = len(test_data_in) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessor = hub.load(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "\n",
    "    # Step 1: tokenize batches of text inputs.\n",
    "    tokenize = hub.KerasLayer(preprocessor.tokenize, name='tokenizer')\n",
    "    tokenized_inputs = [tokenize(text_input), ]\n",
    "\n",
    "    # Step 2 (optional): modify tokenized inputs.\n",
    "    pass\n",
    "\n",
    "    # Step 3: pack input sequences for the Transformer encoder.\n",
    "    seq_length = 50  # We filter out anything less earlier\n",
    "    bert_pack_inputs = hub.KerasLayer(\n",
    "        preprocessor.bert_pack_inputs,\n",
    "        arguments=dict(seq_length=seq_length), name='input_packer')  # Optional argument.\n",
    "    encoder_inputs = bert_pack_inputs(tokenized_inputs)\n",
    "\n",
    "    encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1\", trainable=False, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "#     pooled_output = outputs[\"pooled_output\"]\n",
    "    sequence_output = outputs[\"sequence_output\"]\n",
    "    net = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(10,name='LSTM'),name='Bidirectional')(sequence_output)\n",
    "    net = tf.keras.layers.Dense(64, activation='relu', name='hidden-1')(net)\n",
    "    net = tf.keras.layers.Dense(32, activation='relu', name='hidden-2')(net)\n",
    "    net = tf.keras.layers.Dense(1, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model = build_model()\n",
    "\n",
    "#TODO: See if this improves results\n",
    "initial_learning_rate = 0.1\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=25*TRAIN_STEPS_PER_EPOCH,\n",
    "    decay_rate=0.1,\n",
    "    staircase=True)\n",
    "\n",
    "pre_trained_model.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError(name='mae'),\n",
    "            tf.keras.metrics.MeanAbsolutePercentageError(name='maep')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tokenizer (KerasLayer)          (None, None, None)   0           text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "input_packer (KerasLayer)       {'input_mask': (None 0           tokenizer[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "BERT_encoder (KerasLayer)       {'pooled_output': (N 35068417    input_packer[0][0]               \n",
      "                                                                 input_packer[0][1]               \n",
      "                                                                 input_packer[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "Bidirectional (Bidirectional)   (None, 20)           41840       BERT_encoder[0][8]               \n",
      "__________________________________________________________________________________________________\n",
      "hidden-1 (Dense)                (None, 64)           1344        Bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hidden-2 (Dense)                (None, 32)           2080        hidden-1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "classifier (Dense)              (None, 1)            33          hidden-2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 35,113,714\n",
      "Trainable params: 45,297\n",
      "Non-trainable params: 35,068,417\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_DIR/'model_summary.txt', 'w') as f:\n",
    "    pre_trained_model.summary()\n",
    "    pre_trained_model.summary(print_fn=lambda x: f.write(f'{x}\\n'), line_length=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = OUTPUT_DIR / \"pre_trained_checkpoints\"\n",
    "checkpoint_path = checkpoint_dir / \"{epoch}\"\n",
    "# Create a callback that saves the model's weights every epoch\n",
    "# NOTE: Important to delete the ones we aren't interested in!\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_freq=2*TRAIN_STEPS_PER_EPOCH,\n",
    ")\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(OUTPUT_DIR / 'training.csv',append=True)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, mode='min', restore_best_weights=True)\n",
    "\n",
    "class LearningRateLoggingCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        lr = tf.keras.backend.get_value(self.model.optimizer.learning_rate)\n",
    "        print(f'Learning rate {lr((epoch)*133)}')\n",
    "lr = LearningRateLoggingCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate 0.10000000149011612\n",
      "Epoch 1/100\n",
      "133/133 - 148s - loss: 3729947436777472.0000 - mae: 30009608.0000 - maep: 618.1891 - val_loss: 3457763548069888.0000 - val_mae: 32851384.0000 - val_maep: 1590.1746\n",
      "Learning rate 0.10000000149011612\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: saving model to /rds/general/user/al3615/home/RNN_for_movie_gross_prediction/jobs/pretrained_embeddings/outputs/2021-02-03-1938/pre_trained_checkpoints/2\n",
      "133/133 - 147s - loss: 3236828819750912.0000 - mae: 32934534.0000 - maep: 1550.4521 - val_loss: 3458896882565120.0000 - val_mae: 33544576.0000 - val_maep: 1677.6145\n",
      "Learning rate 0.10000000149011612\n",
      "Epoch 3/100\n",
      "133/133 - 146s - loss: 3237406224416768.0000 - mae: 33048640.0000 - maep: 1562.9899 - val_loss: 3457731067379712.0000 - val_mae: 32868538.0000 - val_maep: 1592.3894\n",
      "Learning rate 0.10000000149011612\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: saving model to /rds/general/user/al3615/home/RNN_for_movie_gross_prediction/jobs/pretrained_embeddings/outputs/2021-02-03-1938/pre_trained_checkpoints/4\n",
      "133/133 - 147s - loss: 3236199607042048.0000 - mae: 33055140.0000 - maep: 1566.1069 - val_loss: 3457948768534528.0000 - val_mae: 32774906.0000 - val_maep: 1580.2826\n",
      "Learning rate 0.10000000149011612\n",
      "Epoch 5/100\n",
      "133/133 - 152s - loss: 3236047404138496.0000 - mae: 33119824.0000 - maep: 1570.2842 - val_loss: 3459443417153536.0000 - val_mae: 32454366.0000 - val_maep: 1538.2644\n",
      "Learning rate 0.10000000149011612\n",
      "Epoch 6/100\n"
     ]
    }
   ],
   "source": [
    "train_start = datetime.now()\n",
    "pre_trained_history = pre_trained_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=test_dataset,\n",
    "    validation_steps=TEST_STEPS_PER_EPOCH,\n",
    "    callbacks=[cp_callback,csv_logger,lr],\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training took {datetime.now()-train_start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover interrupted session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Loading existing checkpoint if needed\n",
    "# home = '/rds/general/user/al3615/home'\n",
    "# model_path = home + '/' + 'RNN_for_movie_gross_prediction/jobs/pretrained_embeddings/outputs/2021-01-24-0401/pre_trained_checkpoints/0044/variables/variables.index'\n",
    "# # pre_trained_model = tf.keras.models.load_model(model_path)\n",
    "# # pre_trained_model.load_weights(model_path)\n",
    "\n",
    "# # latest = tf.train.latest_checkpoint(model_dir)\n",
    "# # print(latest)\n",
    "# checkpoint = tf.train.Checkpoint(model=pre_trained_model) \n",
    "# checkpoint.restore(model_path).run_restore_ops()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class History:\n",
    "#     def __init__(self, h):\n",
    "#         self.history = h\n",
    "\n",
    "# import csv\n",
    "# home = '/rds/general/user/al3615/home/'\n",
    "# training_log = home + 'RNN_for_movie_gross_prediction/jobs/pretrained_embeddings/outputs/2021-01-28-2142/training.csv'\n",
    "# d = dict()\n",
    "# with open(training_log) as f:\n",
    "#     reader = csv.DictReader(f)\n",
    "#     for row in reader:\n",
    "#         for key in row:\n",
    "#             if key not in d:\n",
    "#                 d[key] = list()\n",
    "#             d[key].append(float(row[key]))\n",
    "#     for key in row:\n",
    "#         d[key] = np.array(d[key])\n",
    "# pre_trained_history = History(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = pre_trained_model.predict(train_data_in)\n",
    "pred_test = pre_trained_model.predict(test_dataset)\n",
    "trained_mean = np.mean(train_data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 8), dpi=300)\n",
    "confusion_plot(train_data_out, pred_train, f\"train\")\n",
    "confusion_plot(test_data_out, pred_test, f\"test\")\n",
    "plt.hlines(y=trained_mean, xmin=0.0, xmax=9e8, color='grey', linestyles='dashed')\n",
    "plt.savefig(OUTPUT_DIR / \"pretrained_confusion.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(pre_trained_history, \"loss\")\n",
    "plot_graphs(pre_trained_history, \"mae\")\n",
    "plot_graphs(pre_trained_history, \"maep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up checkpoints we don't want\n",
    "Keeps latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoints_of_interest = list(range(15))\n",
    "# weights_only = False\n",
    "# home = '/rds/general/user/al3615/home/'\n",
    "# checkpoint_dir = home + 'RNN_for_movie_gross_prediction/jobs/pretrained_embeddings/outputs/2021-01-25-0505/pre_trained_checkpoints'\n",
    "# checkpoint_dir = Path(checkpoint_dir)\n",
    "# checkpoints = list(checkpoint_dir.glob(\"[0-9]*\"))\n",
    "# checkpoints.sort(key=os.path.getctime, reverse=True)\n",
    "# checkpoints.pop(0)\n",
    "# if weights_only:\n",
    "#     checkpoints.pop(0)\n",
    "# if checkpoints:\n",
    "#     print(checkpoints)\n",
    "#     for ckpt in checkpoints:\n",
    "#         ckpt_num = int(ckpt.stem)\n",
    "#         if ckpt_num not in checkpoints_of_interest:\n",
    "#             print(f\"Deleting {ckpt_num}\")\n",
    "#             if ckpt.is_dir():\n",
    "#                 shutil.rmtree(ckpt)\n",
    "#             else:\n",
    "#                 ckpt.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2-w-text]",
   "language": "python",
   "name": "conda-env-tf2-w-text-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
